\section{Automated Text Classification}
\label{sec:llm_methodology}

To address the limits of traditional Natural Language Inference (NLI) models in capturing the nuanced legal and technical language of Terms of Service (ToS), this study used an advanced classification pipeline with state-of-the-art Large Language Models (LLMs). Specifically, the pipeline was upgraded from a BERT-based architecture (DeBERTa-v3-large) to the \textit{Gemini 3 Flash} model, accessed via the Google Generative AI API.

\subsection{Model Selection and Rationale}
The choice of \textit{Gemini 3 Flash} was driven by the need for deeper reasoning abilities and better context awareness. Unlike Natural Language Inference (NLI) models, which classify based on entailment probabilities between premises and hypotheses, generative LLMs can interpret complex sentence structures and tell the difference between ambiguous legal standard terms (``General Terms'') and specific geo-arbitrage restrictions. 

Key advantages observed during the model transition included:
\begin{itemize}
    \item \textbf{Contextual Understanding}: The ability to distinguish between benign references to ``account suspension'' (e.g., for fraud) and strategic ``Legal Threats'' tailored to prevent cross-border usage.
    \item \textbf{Zero-Shot Performance}: The model demonstrated high accuracy without extensive fine-tuning, utilizing a robust system prompt to align with the theoretical categories defined in Section 2.
    \item \textbf{Efficiency}: The ``Flash'' architecture provided a high throughput, enabling the processing of the entire dataset (approx. 25,000 sentences) within a reasonable timeframe.
\end{itemize}

\subsection{Operationalization of Constructs (The Coding Scheme)}
Based on the theoretical framework, the following coding scheme was enforced via the LLM system prompt. This scheme maps the abstract concept of "Strategic Response" into measurable data points.

\subsubsection{Strategic Frames}
The model was tasked to identify the underlying justification provided by the firm:
\begin{description}
    \item[Frame: Legal Compliance] Justifying geo-blocking as a non-negotiable legal or contractual necessity (e.g., "Due to licensing agreements...").
    \item[Frame: Security Risks] (Service Provider Frame) Arguments that VPNs/Proxies are unsafe, malicious, or compromise user data.
    \item[Frame: Privacy/Security] (VPN Provider Frame) Arguments focusing on encryption, anonymity, and protection from surveillance.
\end{description}

\subsubsection{Firm Actions}
The model categorized specific enforcement clauses into:
\begin{description}
    \item[Action: Technical Blocking] Active technological measures to detect or block the specific use of VPNs/Proxies (e.g., "We use geo-blocking technology", "Error 403").
    \item[Action: Legal Threat] Explicit threats of account termination, suspension, or legal action specifically for using circumvention tools.
    \item[Action: Account Action] General punitive measures against accounts (termination, suspension) for comprehensive violations.
    \item[Action: Price Discrimination] Explicit differences in pricing based on region, currency, or purchasing power.
    \item[Action: Legitimate Portability] Rules allowing temporary access while traveling (e.g., EU Portability Regulation).
\end{description}

\subsection{Pipeline Architecture and Implementation}
The reclassification process was automated using a customized Python script.

\subsubsection{System Prompt Engineering}
To ensure deterministic and theoretically grounded outputs, the system prompt was engineered with strict constraints. The exact prompt structure is provided below:

\begin{figure}[ht]
\begin{verbatim}
SYSTEM_PROMPT = """You are a scientific classifier for a Thesis on 'Digital Geo-Arbitrage'.
Classify a list of sentences into the provided categories. 

CATEGORIES:
1. Technical Blocking: Measures/Technologies used to detect or block...
2. Legal Threat: Explicit threats of account termination...
3. Security Risk: (Service Provider Frame) Arguments that VPNs are unsafe...
4. Privacy/Security: (VPN Provider Frame) Arguments focusing on encryption...
... [Full List of 10 Categories] ...

INSTRUCTIONS:
- Analyze sentences independently.
- Return a JSON array of objects...
- Format: [ { "category": "Category", "confidence": 0.9 }, ... ]
"""
\end{verbatim}
\caption{System Prompt used for Gemini 3 Flash Classification}
\label{fig:system_prompt}
\end{figure}

\subsubsection{Batch Processing and Error Handling}
To optimize for the API's rate limits and ensure data integrity, the pipeline utilized a batch processing approach. Sentences were grouped into batches of 25 and processed in a single API call. This method significantly reduced network overhead and total processing time.
A robust error-handling mechanism was implemented to manage API timeouts or rate limits (HTTP 429). The script included a ``circuit breaker'' to halt execution upon repeated failures and a resume function to continue processing from the last saved state.

\subsubsection{Data Imputation and Longitudinal Consistency}
A key challenge in analyzing longitudinal Terms of Service data is the sparsity of document updates, as companies do not release new ToS documents every year. To address this, a "Forward-Fill" imputation strategy was applied. If a document was released in a given year (e.g., 2020), its clauses were assumed to remain valid for subsequent years (2021, 2022) until a new document was detected. This ensures the dataset accurately reflects the \textit{active} regulatory environment in any given year, distinguishing between "missing data" and "persistent rules."

Additionally, while "General Terms" (standard legal boilerplate) constitute approximately 94\% of the dataset, they are retained in the master dataset to preserve the full document structure. However, for visual clarity, these terms are frequently excluded from strategic trend graphs to focus on the distinct enforcement categories.

\subsection{Methodological Validation: Gemini vs. Zero-Shot BERT}
To validate the choice of the Gemini 3 Flash model, a comparison was done against a traditional Zero-Shot classification approach using a BERT-based model. The results showed a huge difference between the two models, confirming the need for a modern LLM with large context windows for this specific task, consistent with recent findings on LLM performance in text annotation \parencite{gilardi2023chatgpt}.

\subsubsection{Agreement Analysis}
The comparison revealed an exceedingly low agreement rate of \textbf{26.8\% (Accuracy)} between the two models. Cohen's Kappa score was \textbf{0.032}, suggesting agreement effectively equivalent to random chance. This discrepancy indicated a fundamental difference in how each model interpreted the classification tasks.

\subsubsection{The Core Conflict: Sensitivity vs. Context}
The analysis highlighted two distinct behaviors:
\begin{enumerate}
    \item \textbf{Gemini Performance:} The Gemini model correctly identified that approximately \textbf{91\%} of the dataset consisted of legal boilerplate, categorized as "General Terms." It successfully distinguished specific enforcement clauses from general legal language.
    \item \textbf{BERT Performance:} The BERT model exhibited "Over-Sensitivity," frequently assigning specific strategic tags based on the presence of individual keywords rather than semantic context.
\end{enumerate}

Specific examples of BERT's misclassification included:
\begin{itemize}
    \item \textbf{Legitimate Portability:} BERT flagged 7,853 sentences as "Legitimate Portability" that were merely "General Terms."
    \item \textbf{Account Action:} BERT flagged 6,134 "General Terms" sentences as "Account Action."
\end{itemize}

\textit{Interpretation:} BERT operates on keyword connections, flagging sentences like "You must have an account" as "Account Action." In contrast, Gemini uses reasoning abilities to understand that just mentioning an "account" is standard boilerplate ("General Terms") and saves the "Account Action" tag for sentences that explicitly regulate account termination or suspension.

\subsubsection{Conclusion on Model Selection}
The validation shows that Zero-Shot BERT is not enough for complex legal text analysis without extensive fine-tuning, lacking the nuance needed to tell the difference between just mentioning a topic (e.g., "portability") and its active regulation. Table \ref{tab:model_comparison} and Figure \ref{fig:model_comparison_viz} provide a detailed breakdown of the category distribution discrepancies.

\begin{table}[ht]
    \centering
    \small
    \begin{tabularx}{\textwidth}{l c c c c c}
        \toprule
        \textbf{Category} & \textbf{Gemini \%} & \textbf{BERT \%} & \textbf{Delta} \\
        \midrule
        Technical Blocking & 0.41\% & 0.09\% & +0.32\% \\
        Price Discrimination & 0.48\% & 0.03\% & +0.45\% \\
        Content Licensing & 2.18\% & 5.76\% & -3.58\% \\
        Regulatory Compliance & 2.05\% & 0.41\% & +1.64\% \\
        Legal Threat & 0.47\% & 0.00\% & +0.47\% \\
        Account Action & 0.00\% & 25.89\% & -25.89\% \\
        Legitimate Portability & 0.01\% & 31.99\% & -31.98\% \\
        General Terms & 94.12\% & 26.12\% & +68.00\% \\
        \bottomrule
    \end{tabularx}
    \caption{Model Comparison: Gemini 3 Flash vs. Zero-Shot BERT Classification}
    \label{tab:model_comparison}
\end{table}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/gemini_vs_bert_dist.pdf}
    \caption{Visualizing the Classification Gap: Proportional Category Distribution Between Models.}
    \label{fig:model_comparison_viz}
\end{figure}

Gemini, using its extensive context window and advanced reasoning abilities, performs much better at filtering noise and providing accurate classifications. As a result, Gemini 3 Flash was chosen as the only model for the final analysis.
